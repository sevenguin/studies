## Neural Networks Learning

如何为神经网络拟合参数？从拟合代价函数开始

### Cost function

假设有数据：$\{(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}),…,(x^{(m)},y^{(m)})\}$，L表示神经网络的总层数，$s_l$表示$l$层的神经元数（不包括偏差单元）。

Logistics Regression经过正则化后的损失函数如下：
$$
J(\theta)=-{1\over m}[\sum_{i=1}^my^{(i)}\log h_\theta(x^{(i)}) + (1 - y^{(i)})\log(1-h_\theta(x^{(i)}))] + {\lambda\over 2m}\sum_{j=1}^n\theta_j^2
$$
神经网络的损失函数如下：
$$
J(\theta)=-{1\over m}[\sum_{i=1}^m\sum_{k=1}^Ky_k^{i}\log(h_\Theta(x^{(i)}))_k +(1-y_k^{(i)})\log(1-h_\Theta(x^{(i)}))_k] + {\lambda \over 2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_l+1}(\Theta_{ji}^{(l)})^2
$$

### Back propagation algorithm

最小化损失函数的算法：反向传播算法

为了计算Cost Function的导数项，我们将使用反向传播算法，反向传播算法从直观上来说就是对每一个结点计算这样一项：$\delta_j^{(l)}$=在$l$层第$j$个结点的误差。

### Gradient checking

由于反向传播算法在计算和实现时有很多细小的问题，所以可能不经意间就会遇到一些小错误，这会导致最终的结果出现误差。使用梯度检查可以用来检查在计算过程中是否是正确的。

其实就是对梯度计算进行校验，使用反向传播的方法计算出的梯度值和使用近似值计算出来的梯度值进行比较，如果近似，则说明反向传播计算正确。但是在验证完之后，应该剔除近似值计算这块代码，因为计算量比较大。

近似值的计算公式：
$$
{\delta J_\Theta\over \delta\theta_i} \approx {J_\Theta(\theta_i + \epsilon) - J_\Theta(\theta_i + \epsilon)\over 2\epsilon}
$$
其实就是对参数中某一个进行上下微调，函数值和该参数变化值比值就是近似值，和偏导的理解差不多。

### Random initialization

无论在使用梯度下降方法还是高级的优化方法时，都需要给参数赋初始值。在线性回归或者逻辑回归的时候，经常赋初值为0，但是神经网络中是不可行的。

神经网络中如果初值都是0，则会导致，各个神经元中的各个参数权重相同，这样各个神经元其实就是同一个函数了。

所以初始化每个参数$\Theta_{ij}^{(l)}$为一个随机变量，此值再$[-\epsilon, \epsilon]$。

### 总结

在训练一个神经网络时，需要以下步骤：

1. __确定神经网络的架构：__

   即各个神经元之间的连接模式（如几层，每层几个神经元）；输入层和输出层神经元的个数比较好确定，输入层是特征个数，输出层可以是分类个数；对于隐藏层，默认有一层隐藏层，如果隐藏层数量大于1，则各个隐藏层的神经元数量都相等。

2. __训练神经网络：__

   1. 随机初始化权值
   2. 然后实现向前传递法
   3. 然后计算代价函数
   4. 然后使用反向传播法计算偏导数或偏微分项（代价函数的偏微分）
   5. 使用梯度检查来比较这些已经计算得到的偏导数项把用反向传播算法得到的偏导数值与用数值方法得到的估值进行比较，通过梯度检查来确保两种方法得到的基本接近的两个值。
   6. 最后，使用诸如梯度下降或高级优化方法，与反向传播结合计算参数最优值。

   ### Examples

   自动驾驶