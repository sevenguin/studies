## note14-Dimensionality Reduction

### 降维

降维的两个目：压缩数据和可视化

#### 压缩数据

从2D降到1D，如果2D的两个特征之间存在线性关系，则可以将这两个特征的数据映射到这条线上，称为1D数据，这样也可以压缩数据（只用一个线性表达式和一维数据即可）。

类似2D转为1D，如果数据集中显示在一个平面上，则可以将其降维到一个新的2D坐标中。

这里的压缩可以帮助后期更好的计算和算法拟合。

#### 可视化

降维后可视化可以帮助我们更有效的了解数据

### PCA(主成分分析)

PCA就是寻找一个投射平面，对所有数据进行投影（达到降维的目的），使得投影误差（就是原数据点到投影平面的距离）达到最小。

一般在使用PCA之前，先对数据进行归一化和规范化。

将n维数据降维到k维：找k个向量$u^{(0)},u^{(1)},…,u^{(k)}$，用来表示数据，使得投影误差（Projection Error）最小。和线性回归不同的是点到线的距离方式不同，一个是垂直某个坐标轴（线性回归），一个是垂直找到的线（PCA）。

#### PCA算法

训练集为：$x^{(1)},x^{(2)},…, x^{(1)},x^{(m)}$

在进行PCA之前，先对数据做预处理（均值标准化(mean normalization)或特征缩放(features scaling) ：

* 均值标准化：$x^{(i)}_j=x_j - \mu_j，\mu_j={1\over m}\sum_{i=1}^mx_j^{(i)}$，是对某个特征逐一进行均值标准化
* 特征缩放：如果哥哥feature有不同范围，则需要将他们进行缩放：${x_j - \mu_j\over s_j}，s_j可以取各个特征的标准差$


PCA算法需要干两件事，其一就是找到降维之后的向量空间，其二就是降维之后的数据在新的向量空间中的表现。例如2D转为1D，需要找到1D空间下的那条线，然后找到2D空间点到1D空间这条线上对应映射点。

__算法描述：__

假设将数据从n维降维到k维，先计算协方差矩阵（covariance matrix），如下：
$$
\Sigma = {1\over m}\sum_{i=1}^n(x^{(i)})(x^{(i)})^T，n为维度数
$$
计算$\Sigma$的特征向量（eigenvectors），选取$\Sigma$的奇异值分解（SVD）中的矩阵U的前K个向量组成$U_{reduce}$

计算$Z=U_{reduce}^TX$

### Choosing the Number of Principal Components-K

PCA视图最小化$J={1\over m}\sum_{i=1}^m||x^{(i)} - x_{approx}^{(i)}||^2$，即x原点到x在低维空间的映射点的距离。

所有数据总的方差$V={1\over m}\sum_{i=1}^m||x^{(i)}||^2$，数据距离0点的距离，表示数据的波动性。

则会选择k值，使得下面式子成立（0.01可选，一般设定为0.01）：
$$
{J\over V}\le 0.01/0.05\ \ \ \ \ \ \ \  \ (1\%/5\%)
$$
用PCA的语言来说就是：保留原数据99%/95%的方差性。

一个算法：根据上面的验证，从k=1开始验证，直到选择的保留主成分（方差）满足上面的不等式。

但是其实在计算$SVD(\Sigma)$时，有一个S值，上面的不等式${J\over V}$可以用下面式子替换：
$$
R=1-{\sum_{i=1}^kS_{ii}\over sum_{i=1}^nS_{ii}}
$$
所以一次SVD计算，就可以使用S矩阵就可以循环计算（还是从1开始计算）：$R\le 0.01/0.05$。

### Restruction from compressed representation

既然上面说PCA可以作为压缩，那如何从压缩数据中恢复出原来的数据呢？

可以得到X的近似值$X_{approx}=U_{reduce}Z$

