## Neural Networks

### Non-linear hypotheses

在非线性分类问题时，如果特征数量n很大，则很容易出现NP问题。如果使用Logistics回归解决，则需要增加二次项、三次项，则复杂度呈指数增长($O(n^k))$。结果就是难以计算，而神经网络则可以更好的处理这类问题。

### Neurons and the Brain

最开始是为了制造能够模拟大脑的机器，兴起于80~90年代，90年代后期应用又减少了。可能由于神经网络是一类运算量偏大的方法，所以最近今年由于计算机发展，计算量已经不是瓶颈，所以又重新流行起来。而且由于其他一些技术因素的改善，一些应用中，神经网络是最先进的技术。 

### Model representation

如何表示神经网络？在使用NN时，如何表示我们的假设或模型。

#### 使用数学方式来表示神经网络

神经网络是模拟大脑中的神经元或神经网络时发明的，神经元作为信息处理和传输的最小单元，神经网络在模拟时，使用一个逻辑单元模拟一个神经元。

激励函数：只是对类似非线性函数g(z)的另一个术语称呼，$g(z)={1\over 1+e^{-\theta^Tx}}$

神经网络第一层为输入层，特征数据的输入，最后一层为输出层，中间的层称为隐藏层。

使用：

$a_i^{(j)}$：表示第j层的第i个神经元，即第j层的第i个激励。这里的激励是指有一个具体的神经元读入

$\Theta^{(j)}$：从第j层映射到第j+1层的权重矩阵控制函数

例如一个输入层有三个单元（$x_1,x_2,x_3$)，隐藏层有三个单元（$a_1^{(2)},a_2^{(2)},a_3^{(2)}$)，最后产生一个输出($h_\Theta(x)$的神经网络，计算表示如下：

$a_1^{(2)}=g(\Theta_{10}^{(1)}x_0 + \Theta_{11}^{(1)}x_1 + \Theta_{12}^{(1)}x_2) + \Theta_{13}^{(1)}x_3$

$a_2^{(2)}=g(\Theta_{20}^{(1)}x_0 + \Theta_{21}^{(1)}x_1 + \Theta_{22}^{(1)}x_2) + \Theta_{23}^{(1)}x_3$

$a_3^{(2)}=g(\Theta_{30}^{(1)}x_0 + \Theta_{31}^{(1)}x_1 + \Theta_{32}^{(1)}x_2) + \Theta_{33}^{(1)}x_3$

$h_\Theta(x)=a_1^{(3)}=g(\Theta_{10}^{(2)}a_0^{(2)}+\Theta_{11}^{(2)}a_1^{(2)}+\Theta_{12}^{(2)}a_2^{(2)}+\Theta_{13}^{(2)}a_3^{(2)})$

如果神经网络在第j层有$s_j$个单元，在第j+1层有$s_{j+1}$个单元，那么$\Theta^{(j)}$的维度是：$s_{j+1}\times(s_j+1)$，例如上面$\Theta^{(1)}$的维度就是$3\times4$。

#### 向量实现以及计算方法

假设上面的计算表示中，有：

$a_i^{(j)}=g(z_i^{(j)})$

$z_i^{(j+1)}=\Theta^{(j)}x$，将z向量化，所以上面的计算表示可以通过向量表示为：

$z^{(2)}=\Theta^{(1)}x，a^{(2)}=g(z^{(2)})$，a和z都是三维向量，g就是对z的元素一一做映射得到a。$a_0^{(2)}$叫做偏置单元，设置为1.

则$z^{(3)}=\Theta^{(2)}a^{(2)}，h_\Theta(x)=a^{(3)}=g(z^{(3)})$

上面的计算过程也叫做向前传播法。

表示神经网络中神经元相连接的方式称为神经网络的架构，所以神经网络架构是指不同的神经元是如何相互连接的

### 示例

假设没有隐藏层的神经网络表示AND操作，输入空间$x_1,x_2$取值{0，1}，则$\Theta^{(1)}=[30,20,20]$，这样得到的$h_\Theta(x)$就是AND操作了。

那么可以通过多层神经网络来表示XOR或者NXOR问题。

#### 多类分类的应用

如果需要分类k个分类，则输出层有k个神经元，每一个神经元表示是否是$y^{(i)},i\in[0,k)$，所以输出的结果是一个向量，只有一个元素值为1，这个元素的位置就表示实际的分类结果是哪一类。

