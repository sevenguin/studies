#Boosting
boosting（提升）方法是一种常用的统计学习方法，应用广泛且有效，在分类问题中，它通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类的性能。

## AdaBoost算法
###提升方法的基本思路
提升方法基于这样的假设：对一个复杂的问题，将多个专家的判断进行适当的综合所得到判断，要比其中任何一个专家单独的判断好。

强可学习：如果存在一个学习算法能够学习它，并且正确率很高，则称为强科学习；    
弱科学习：如果学习算法正确率只比随机策略好一点，则称为弱可学习。

对于分类问题而言，给定一个样本集，找到弱分类器相比找到强分类器更容易，提升方法就是从弱分类器触发，反复学习，得到多个弱分类器，然后组合这些弱分类器，构成一个强分类器。大多数提升方法都是改变样本数据的概率分布（训练数据的权值分布），针对不同的训练数据分布调用弱学习算法学习一系列弱分类器。

对于上面的描述，有两个问题：

1. 如何改变样本数据的概率分布？
2. 如何将一系列弱分类器构建成一个强分类器？

AdaBoost对上面两个问题的解决方案：

1. 每一次循环时，提升错误分类数据的权值，降低正确分类的权值
2. 对每一个弱分类器赋权值，对正确率高的赋更高的权值，使其在投票中权值更大，反之更小

###AdaBoost算法描述
输入：训练数据集$T={(x_1, y_1), (x_2, y_2)...(x_n, y_n)}，x_i\in X \subseteq R^n，y_i\in Y={-1, +1}，X是实例空间，Y是标记集合$
输出：最终分类器G(x)
步骤：

1. 设置初始权值：$D_1={w_{1,1}... w_{1,i}...w_{1,n}}，w_{1,i}={1\over n}，\sum_i^nw_{mi}=1，1表示初始的分类器，i=1,2...n$
2. 对m=1,2,3...M
	* 使用具有权值分布$D_m$的训练数据集学习（初始就是$D_1$)，得到基本分类器:$G_m(x)$
	* 计算分类器$G_m(x)$在当前权值分布的数据集上的分类误差率
	$\qquad e_m=P(G_m(x_i)\neq y_i)=\sum_{i=1}^nw_{m,i}I(G_m(x_i)\neq y_i)=\sum_{G_m(x_i)\neq y_i}w_{w,i}，I是指示函数，\\\
	\qquad \qquad I(arg)，如果arg为True值为1，否则为0$
	* 计算$G_m(x)$的系数
	$\qquad a_m={1\over 2}ln{1-e_m\over e_m}，当e_m\le{1\over2}时，a_m随着e_m减小而增大，\\\
	\qquad 即错误率高的分类器在最终分类器构造中权值更高，e_m\gt{1\over 2}那还不如瞎猜呢$
	* 更新训练集的权值分布
  $\qquad D_{m+1}=(w_{m+1,1}....w_{m+1, i}...w_{m+1, n})\\\
  \qquad w_{m+1, i} = {w_{m,i}\over Z_m}e^{(-a_my_iG_m(x_i))}，G_m(x_i)=y_i则y_iG_m(x_i)=1，否则=-1，\\\
  \qquad  所以：G_m(x_i)=y_i时，w_{m+1,i}={w_{m,i}\over Z_m}e^{-a_m}，e^{-a_m}<1，说明权值变小\\\
  \qquad \qquad G_m(x_i)\ne y_i时，w_{m+1,i}={w_{m,i}\over Z_m}e^{a_m}，e^{a_m}>1，说明权值变大\\\
  \qquad Z_m是规范化因子：Z_m=\sum_{i=1}^Ne^{(-a_my_iG_m(x_i))}
$
3. 构建分类器的线性组合：
$f(x)=\sum_{m=1}^Ma_mG_m(x)\\\
最终得到分类器：G(x)=sign(f(x))=sign(\sum_{m=1}^Ma_mG_m(x))，G_m(x)可能为-1（y为1或-1）
$
G(x)具体选那个分类方法还不一定。
