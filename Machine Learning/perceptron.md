#Perceptron
__感知机（perceptron）__是二类分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，取+1和-1二值。感知器对应于输入空间（特征空间）中将实例分为正负两类的分离超平面，属于判别模型。感知机学习旨在根据训练数据求出将其线性划分的超平面，为了判别求得的超平面的准确、可行性，采用梯度下降等方法求损失函数的极小值。    
感知机分为原始形式和对偶形式，感知机也是神经网络与支持向量机的基础，1957年由Rosenblatt提出。    
###感知机模型
定义：    
假设输入空间（特征空间）是X属于空间R^n，输出空间是Y={+1，-1}，x是X实例的特征向量，对应于输入空间的点，输出y表示Y的实例类别。从输入空间到输出空间的函数：$f(x)=sign(w\cdot x + b)$称为感知机。其中w,b是感知机的参数，w也属于空间R^n，w称作权值或权值向量，b叫做偏置（bias），w·x表示w和x的内积，sign函数：$sign(x) = +1\  if\  x>=0\ else\ -1$ 

###感知机策略
__线性可分性：__如果对于数据集D={(x1,y1),(x2,y2)...(xn,yn)}，y空间为{+1, -1}，如果存在超平面w·c+b=0能够将数据集正确的划分，即对于w*xi+b>0则yi=+1，反之亦然。则称为数据集线性可分，否则数据集D线性不可分。
####感知机学习策略
假设数据集是线性可分的，则感知机的目标就是求得一个能将数据集分开的超平面，即求得w·x+b中的w和b。需要定义一个学习策略，即定义损失函数并将损失函数最小化。由于目标是求得w和b，所以损失函数最好是对w和b连续可导，这样有助于求得最值。（其实误分数据个数可以作为很好的损失函数）所以选择损失函数是误分类点到超平面S的总距离（这个其实有个缺点就是，如果有一个点离S很远，会对算法造成一定伤害，因为距离近的很多点可能被误分，由于为了降低距离而降很远的点分对），损失函数：
$$
任一点到平面距离：

{1\over||w0||}|w\cdot x0 + b|, \qquad||w||就是向量的模=\sqrt(x * x^T)，向量个元素平方和的开方；\\

*Ln范数：向量各元素绝对值的n次方求和然后求1/p次方\\

由于误分的点的结果和实际结果符号相反，所以(w\cdot x_i+b)\cdot y_i <0，所以所有误分类数据距离超平面总距离为：\\

L(w,b)=-{1\over ||w||}\sum_{x_i\in M}y_i(w\cdot x_i+b)，这样可以避免使用绝对值
$$
感知机的策略就是在假设空间总选取使损失函数L(w,b)选取损失函数最小的模型参数w,b，即感知机模型。

###感知机学习算法
感知机学习问题转化为求解损失函数的最小化问题，而最小化问题使用随机梯度下降算法来求解。
\\[
\min_{w,b}L(w,b)=\sum_{x_i\in M}y_i(w\cdot x_i+b)，这样可以避免使用绝对值\\\
把-{1\over ||w||}去掉是为什么？
\\]

数据如果是线性可分，则感知机收敛迭代算法是收敛的，有一个最大误差值（误分类次数有最大值），如果数据不线性可分，则感知机学习算法不收敛，迭代结果会发生震荡。感知机算法即使收敛也可能因为迭代次数、初值选择、数据迭代顺序不同而产生不同的解。