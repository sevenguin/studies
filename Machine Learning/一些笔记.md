# 一些笔记

先做零散的记录，后续自己梳理好了，再将这些笔记整理成文章。

### Subset Selection

因为以下两个原因，最小二乘法并不能满足我们的需求：

1. __Prediction accuracy(预测精度):__从最小二乘法的目标函数就能发现，其追求的是最小的偏差（即和训练数据之间的差值最小），但是这可能会造成较大的方差（预测值很分散，如果对数据稍作改动，则预测结果可能会变化很大，这不符合实际情况，而且预测值之间造成较大方差——Ax=b，A的行向量线性相关就会造成这样的问题，如果行数<列数，则预测值会有很大的跳动）。prediction accuracy可以通过缩小或设置某些系数为0（即减弱或剔除某些列的影响）。这样我们会增大偏差而减小方差，以达到提升整体prediction accuracy的目的。
2. __Interpretation(可解释性):__如果有大量的预测因子（即列数，维度，predictor），而很多预测因子可能根本和预测目标无关或者相关性很弱，这时候选择最有效的预测因子子集就很必要，能增强对预测结果的解释性，也可以降低整个算法的过拟合现象。

子集选择可以保留变量的一部分，消除剩下的部分，最后还是通过最小二乘法（Least squares regression）来使用保留的那部分来预测系数。下面是几个针对线性回归的subset selection方法。

#### Best-Subset Selection

Best-Subset Selection是找每一个$k\in {0,1,2,...,p}，p是总的预测因子个数$，k是作为OLS计算的因子个数。__leaps and bounds__（Furnival and Wilson, 1974）算法可以解决p取值为30到40之间的问题。

如何选择k其实是在偏差和方差之间做平衡，以及算法的简约性。有一些准则我们用来评测subset的效果，一般情况，我们选择能够在最小化预测误差中选择最小的集合。还有交叉验证来估计预测误差、选择k，AIC也是一个很好的选择。

#### Forward- and Backward-Stepwise Selection

相比于搜索所有的可能子集（p超过40就没办法了，NP问题），Forward-Stepwise selection由截距开始，然后从剩余因子里面连续的增加能够更好的适合算法的因子到subset中。

Backward Stepwise Selection由所有数据开始，然后逐步一个一个删除预测因子——对算法fit影响最小的因子。

Backward只能在N>p时使用，而Forward没有限制。

#### Forward-Stagewise Regression

FS(Forward-Stagewise)相对stagewise来说有更多的限制，其实和stagewise一样，也是从intercept开始，不过intercept设置为$\overline y$，所有系数初始化为0。计算每个维度与残差相关度，选取相关系数最大的维度。

