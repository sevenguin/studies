#Naive Bayes
朴素贝叶斯算法是基于数据特征独立的假设以及贝叶斯定理来进行分类的算法。对于给定数据，首先基于特征条件独立假设，学习输入、输出的联合概率分布，然后基于次模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。

朴素贝叶斯分类时，对给定的输入x（向量，可能有m个特征），通过学习到的模型计算后验概率分布:
\\[P(Y=c_k|X=x)\\]
将后验概率最大的c_k作为x的类输出，后验概率计算根据贝叶斯定理计算得到：
\\[
P(Y=c_k|X=x) = {P(X=x|Y=c_k) \cdot P(Y=c_k)\over P(X=x)}\\\
P(X=x)对所有的c_k都相同，所以朴素贝叶斯分类器可以表示为：y=f(x)=arg\max_{c_k}P(Y=c_k)\cdot P(X=x|Y=c_k)\\\
由于x的m个特征根据假设是相互独立，所以P(X=x|Y=c_k)=\prod_jP(X^{(j)}=x^{(j)}|Y=c_k)\\\
P(Y=c_k)和P(X^{(j)}=x^{(j)})可以使用最大似然估计来求得具体的概率值。
\\]

由于上面计算时各个特征概率的极大似然估计得到的概率相乘得到P(X=x|Y=c_k)，所以有个问题，如果其中一个特征在Y=c_k下是0，则整个概率就是0，这可能会造成最后分类的偏差。所以为了解决这个问题采用贝叶斯估计，条件概率贝叶斯估计是：
\\[
P_\lambda(X^{(x^{(j)}=a_{jl}|Y=c_k)={\sum^N_{i=1}I(x_i^{(j)}=a_{jl},y_i=c_k) + \lambda \over \sum^N_{i=1}(y_i=c_k)+S_j\lambda}\\\
其中S_j表示第j个特征取不同值的个数，在具体的x作为输入时，a_{jl}其实就是x^{(j)},\\\
当\lambda=0时，就是极大似然估计，\lambda=1时，这时称为拉普拉斯平滑（Laplace smoothing）。\\\
同样先验概率的贝叶斯估计是：
P_\lambda(Y=c_k)={\sum^N_{i=1}I(y_i=c_k)+\lambda\over N+K\lambda}
\\]
