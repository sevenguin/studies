#Logistic Regression
##Logistic回归模型
Logistic分布（Logistic distribution）：    
设X是连续随机变量，X服从Logistic分布是指X具有下列分布函数和密度函数：    
$
F(x)=P(X\le x)={1\over{1+e^{-(x-u)/r}}}\\\
f(x)=F^\prime(x)={e^{-(x-u)/r}\over r(1+e{-(x-u)/r})^2},其中，u为位置参数，r>0为形状参数
$
###二项Logistic回归模型
二项Logistic回归模型是一种分类模型，由条件概率分布P(Y|X)表示，形式为参数化的Logistic分布，这种情况下Y取值为1或0.通过监督学习的方法来估计参数。     
二项Logistic回归模型概率分布：
$
P(Y=1|x) = {exp(w\cdot x+b) \over 1 + exp(w\cdot x+b)}\\\
P(Y=0|x) = {1 \over 1 + exp(w \cdot x + b)}, w称为权值向量，b为偏置，w\cdot x 为内积
$
计算两个概率值，然后类别属于概率大的那一类。    
####回归模型的特点
几率=p/(1-p), 一个事情的几率是这个事情发生的概率p除以它不发生的概率(1-p)，该事件的对数几率（log odds）或logit函数是：
$
logit(p) = log{p \over 1 - p}\\\
Y=1的对数几率为 log{P(Y=1|x\over 1 - P(Y=1|x)}=w\cdot x\\\
也就是Y=1的对数几率其实就是参数w和x（这里把b也看做x，b的参数看做1）的线性函数，\\\
也就是可以将w \cdot x转换为概率
$
当上面的线性函数值趋于正无穷大时，概率值就越接近1；反之，线性函数值趋于负无穷时，概率趋于0。    
####模型参数估计
设：$P(Y=1|x)=f(x), P(Y=0|x)=1-f(x)，f(x)={e^{w\cdot x}\over 1 + e^{w\cdot x}}$
则，似然函数为：$\prod_{i=1}^Nf^y_i(x_i)\cdot (1-f(x_i)^{1-y_i})$
求此值最大，即求对数似然函数最大：
$L(w) = ln(\prod_{i=1}^Nf^{y_i}(x_i)\cdot (1-f(x_i)^{1-y_i}))\\\
\qquad=\sum_{i=1}^N[y_ilnf(x_i) + (1-y_i)ln(1-f(x_i))]\\\
\qquad=\sum_{i=1}^N[y_iln{f(x_i)\over 1-f(x_i)} + ln(1-f(x_i))]\\\
\qquad=\sum_{i=1}^N[y_i(w\cdot x_i) - ln(1 + e^{w\cdot x_i}]
$
对上面式子使用梯度下降方法或者拟牛顿方法来求得w最好估计$w^\prime$。最后按照计算$P(Y=1|x, w=w^\prime)和P(Y=0|x, w=w^\prime)$
###多项Logistic回归模型
可以将二项回归模型推广到多项Y={1,2,3...K}，则：
$
P(Y=k|x) = {e^{w_k \cdot x}\over 1 + \sum_{k=1}^{K-1}e^{w_k \cdot x}}，k=1,2,3...K-1\\\
P(Y=K|x) = {1 \over 1 + \sum_{k=1}^{K-1}e^{w_k \cdot x}}
$
模型参数估计参考二项，还是使用似然函数估计，只不过在推导的时候替换掉多项的概率的公式。
##最大熵估计
###最大熵原理
最大熵原理认为，学习概率模型时，在所有可能的概率模型中，熵最大的模型是最好的模型。通常用约束条件来确定概率模型集合，所以最大熵原理也可以表述为在满足约束条件的模型集合中选取熵最大的模型。
假设离散随机变量X的概率分布是P(X)，则其熵是：
$H(P)=-\sum_xP(x)logP(x)，熵满足：0\le H(p)\log|X|，|X|是x的取值个数$
最大熵原理认为要选择概率模型首先必须满足已有的事实，即约束条件，在没有多信息的情况下,那些不确定的部分都是等可能的。最大熵原理来表示等可能性（上面的那个不等式在P(x)都相等时熵最大，右边等号成立）。注意，要在满足约束条件下达到等可能性。
###最大熵模型
条件熵：$H(Y|X)=\sum_{i=1}^n p_iH(Y|X=x_i), p_i=P(X=x_i)即X的各个分类在数据集中占比, i=1,2,...,n$

##概念：
__似然函数__    
似然函数是一种关于统计模型中的参数和函数，表示模型参数中的似然性。似然性和概率意思相近，都是指某种事件发生的可能性。在统计学中，似然性和概率又有点不同。概率指在已知一些参数的情况下某件事发生的概率；而似然性则是已知某件事发生的概率而估计有关事情的性质的参数。
已知A事件发生，运用似然函数，我们估计参数B的可能性。
$最大似然函数：L(\theta|A)=P(\theta|A)={P(\theta,A)\over P(A)}=P(A|\theta)，
\\\其实应该是P(A,\theta)，\theta是参数而不是条件。A已发生，P(A)=1\\\
P(A|\theta=\theta_i)=\prod_{i=1}^NP(y_1, y_2, ..., y_R|x_i, \theta)\\\
求得这个值最大，则是最优\theta值。一般会转换成对数似然函数，即：ln(L(\theta|A))
$

__拉格朗日乘数法__    
在数学最优问题中，拉格朗日乘数法是一种寻找变量受一个或多个条件限制的多元函数的极值的方法。这种方法将一个有n个变量和k个约束条件的最优化问题转换为一个有n+k个变量的方程组的极值问题，其变量不受任何约束。这种方法引入了一种新的标量未知数，即拉格朗日乘数。例如：z=f(x,y),条件是g(x,y)=0，则在满足g(x,y)=0的极值时，先做拉格朗日函数：$F(x,y,\lambda)=f(x,y)+\lambda g(x,y)$，然后对$F(x,y,\lambda)$各个变量（x,y,$\lambda$）求偏导，然后偏导等于0，求出各个变量值，如果值满足约束条件，那就有可能是约束条件先的极值点。

概率论的东西需要看看了，否则概率论相关的算法就看不下去了。
