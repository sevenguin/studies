## K-NN

knn算法描述上比较简单，但是对于数据集比较大的情况下每次都计算到所有数据的距离显然耗费比较大。所以这里主要说一下kd树方法，用来提高k近邻搜索的效率。

#### kd树构建算法：

假设有数据集$D={(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)}$，$x_i$维度为$p$，则：

1. 从$p$维中选择方差最大的维度作为分割维度，设其为$s$，这一步需要计算每一个$x^{(j)},j=1,2,...,p$；
2. 然后对选定维度$s$排序，取其中值$x^{(s)}_m$作为分割，在$s$维度上，所有数据大于$x^{(s)}_m$的分到右边，小于$x^{(s)}_m$的分到左边，等于$x^{(s)}_m$的则存起来，算入该节点的数据集中
3. 然后对左、右两边数据进行第1、2步操作，直到所有数据都归入某个节点为之。

> 第1步中的维度选择，为了避免麻烦，可以顺序循环的选各个维度

#### kd树搜索

设已经构建了树$T$，现在给定点$x$，需要找$x$的近邻：

1. 按照每个节点拆分所使用的维度，小于节点分割值（当时选择的中值）去左子树搜索，大于节点分割值的去右子树搜索（等于的情况可以放在左边或右边搜索），直到找到一个叶结点；
2. 设这个叶结点为最近邻点，然后以$x$点和当前最近邻点（叶结点）的距离为半径（设为$r$）画圆，寻找是否有更近的点。具体的方法是：判断$x$到该叶节点的父节点的分割值在父节点所选的维度上差值是否大于半径$r$，如果大于则说明没必要再去找父节点的另一个子树的节点，如果是小于则去判断父节点另一棵子树中的节点的距离是否更小。如果存在，则更新最近邻距离为该节点，然后重复此步骤，如果不存在，则继续向上查找（父节点的父节点）。

