#SVD
Sigular Value Decomposition

### 概念介绍

奇异值分解是对实、虚矩阵进行分解，分解成几个矩阵相乘。奇异值分解大小为(m,n)的实、虚矩阵$\textbf M$：将其分解成$\textbf U\Sigma \textbf V^T$格式，这里$\textbf U$是(m,m)的酋矩阵（如果是实矩阵则为正交矩阵），$\Sigma$是(m,n)对角矩阵（对角线元素为非负数，并且都是实数），$\textbf V$是(n,n)的酋矩阵，$\Sigma中的元素\sigma_i$叫做矩阵$\textbf M$的奇异值，$\textbf U$的列向量叫做$\textbf M$的左奇异向量，$\textbf V$的列向量叫做$\textbf M$的右奇异向量。

奇异值分解的计算可以使用下列方法来计算：

* 左奇异向量$\textbf U$，是一组$\textbf M\textbf M^T$的正交特征向量
* 右奇异向量$\textbf V$，是一组$\textbf M^T\textbf M$的正交特征向量
* $\textbf M$的非0奇异值（$\sigma_i$)是$\textbf M\textbf M^T$和$\textbf M^T\textbf M$的特征值的平方根

一个惯例就是按降序排列奇异值，所以对于矩阵$\textbf M$，其$\Sigma$矩阵是唯一的。

### 直观解释

#### $\textbf M$是方阵

特殊情况下$\textbf M$是(m,m)的实数方阵，且行列式>0，$\textbf U,\textbf V^T和\Sigma$都是实数(m,m)的矩阵，$\Sigma$可以看做是收缩矩阵(scaling matrix--对矩阵进行缩放或放大，每个方向上可能都一样也可能都不同)，而$\textbf U，\textbf V^T$可以被看做是旋转矩阵（在空间中旋转）。所以从几何角度来看，$\textbf U\Sigma\textbf V^T$可以看做是三个几何转换的组合：一个旋转；一个收缩；另一个旋转。这也就解释了剪切矩阵（shear matrix：一个基础矩阵，表示将某一列或行的$\lambda$倍加到另一行上，一般就是$\textbf I$矩阵，将其中的一个0元素替换成$\lambda$）为什么能被描述成一个序列。

##特征值
特征值分解和奇异值分解的目的都是一样，就是提取一个矩阵最重要的特征，先介绍下特征值分解。
$$
Av=\lambda v\\
v称为矩阵A的特征向量，\lambda称为特征值。
$$
一个矩阵的一组特征向量是一组正交向量，特征值分解是将一个矩阵分解成下面的形式：
$$
A=Q\Sigma Q^{-1}，Q是矩阵A的特征向量组成的矩阵，\Sigma是一个对角阵,\\
对角阵上每个元素就是一个特征值。
$$
一个矩阵其实就是一个线性变换，因为一个矩阵乘以一个向量后得到的向量其实就是相当于将这个向量进行线性变换。特征值就表示矩阵变化的主要变化。但是特征分解需要矩阵是方阵。



##正则化
在构建模型时，如果选择的复杂度较大（参数较多），则容易出现过拟合，在模型从简单到复杂过程中会形成一个模型空间，如何在这个空间中选择一个最好的模型（即避免过拟合，又可以较好的拟合数据），正则化就是一个好方法。
正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或者罚项，正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大。正则化一般的形式如下：
$$
\min_{f\in F}{1\over N}\sum_{i=1}^NL(y_i,f(x_i)) + \lambda J(f)，其中前面一项为模型的误差函数，\\后面一项就是正则化项，\lambda为他们之间的系数，如果第一项是平方差的话，\\

J(f)可以是参数的L_2范式，例如：

L(w）={1\over N}\sum_{i=1}^N(f(x_i;w) - y_i)^2 + {\lambda\over 2}||w||^2
问题：
$$

1. 矩阵的特征向量就是正交向量么？
2. 正交向量的特性
3. 特征值和特征向量的意义