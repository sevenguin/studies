#SVD
Sigular Value Decomposition
##特征值
特征值分解和奇异值分解的目的都是一样，就是提取一个矩阵最重要的特征，先介绍下特征值分解。
\\[
Av=\lambda v\\\
v称为矩阵A的特征向量，\lambda称为特征值。
\\]
一个矩阵的一组特征向量是一组正交向量，特征值分解是将一个矩阵分解成下面的形式：
\\[
A=Q\Sigma Q^{-1}，Q是矩阵A的特征向量组成的矩阵，\Sigma是一个对角阵,\\\
对角阵上每个元素就是一个特征值。
\\]
一个矩阵其实就是一个线性变换，因为一个矩阵乘以一个向量后得到的向量其实就是相当于将这个向量进行线性变换。特征值就表示矩阵变化的主要变化。但是特征分解需要矩阵是方阵。
##奇异值
奇异值分解是一个能适用于任意矩阵的分解方法：
\\[
A=U\Sigma V^T,假设A是一个n*m的矩阵，那么得到的U是一个n\*n的方阵（里面的向量是正交的，U里面\\\的向量称为左奇异向量），\Sigma是一个n\*m的矩阵(除了对角线元素都是0，对角线上的元素称为奇异值)，\\\V^T是一个n\*n的矩阵（里面的向量也是正交，V里面的向量称为右奇异向量）。
\\]


##正则化
在构建模型时，如果选择的复杂度较大（参数较多），则容易出现过拟合，在模型从简单到复杂过程中会形成一个模型空间，如何在这个空间中选择一个最好的模型（即避免过拟合，又可以较好的拟合数据），正则化就是一个好方法。
正则化是结构风险最小化策略的实现，是在经验风险上加一个正则化项或者罚项，正则化项一般是模型复杂度的单调递增函数，模型越复杂，正则化值就越大。正则化一般的形式如下：
\\[
\min_{f\in F}{1\over N}\sum\_{i=1}^NL(y_i,f(x_i)) + \lambda J(f)，其中前面一项为模型的误差函数，\\\后面一项就是正则化项，\lambda为他们之间的系数，如果第一项是平方差的话，\\\
J(f)可以是参数的L_2范式，例如：
L(w）={1\over N}\sum\_{i=1}^N(f(x_i;w) - y_i)^2 + {\lambda\over 2}||w||^2
\\]
问题：

1. 矩阵的特征向量就是正交向量么？
2. 正交向量的特性
3. 特征值和特征向量的意义