#概率论与数理统计
## 统计

统计的目标就是通过从整体中抽样得到的样本来对整体（population）做推论（往往是提供决策意见，降低决策产生的成本以及最优的使用获得的信息），并且提供一种好的推论方法。

##事件的概率

###概率是什么
主观概率，可以理解成一种心态或倾向，例如个人对天气下雨可能性判断。这种主要根据，1、个人经验；根据利害关系（如自己没带伞可能就更倾向于预测不下雨）
事件，基本事件可组成新事件，可描述的试验称为一个事件；随机事件、偶然事件；必然事件和不可能事件。
古典概率：如果一个试验有N个可能结果，而事件E恰好包含其中M个结果，则事件E的概率记做：P(E)=M/N
概率的统计定义：一事件出现的可能性大小，应由在多次重复试验中其出现的频繁程度去刻画。这时，频率只是概率的估计而非概率本身。在无数次重复时，假设频率会在实际概率值的很小的邻域内摆动，实验次数越多则邻域越小，如果是无限次，则就等于。——极限问题
假设检验：设根据一定的理论、假设算出某个事件A的概率为p，则这个理论假设是否与实际相符？可以诉诸实验，进行大量重复的试验观察事件A的频率m/n，如果m/n与p相近，则认为实验结果支持了相关理论，反之则有误。

概率的公理化定义
柯尔莫哥洛夫提出概率论公理化（1933年）
公理描述如下：

> 设基基本事件组成集合$\Omega$，一个事件由若干基本事件组成，则$\Omega$的子集（包括自身和空集）构成一个新的集合F，F中每个成员就称为事件，事件有概率，其大小随事件而异，即概率是事件的函数，与此相应，柯氏公理体系中引入一个定义在F上的函数P，对F中任一成员A，P(A)表示为事件A的概率。对P有一下限制：P(A)值域为[0,1]；$P(\Omega)=1, P(\Phi)=0$;加法公理(互斥事件值和的概率等于各事件的概率之和

公理是为了构成一个完备体系所需要的最少的前提假设或者逻辑基础。

###古典概率计算
排列计较次序而组合不计较
排列：$P_r^n = {n *(n-1)*....* (n-r+1)}={n!\over (n-r)!}$
组合：$C_r^n={P_r^n\over r!}$
$0!=1$
r限制为非负整数，n可以为负数
###事件的运算、条件概率与独立性
一般问题中有许多比较简单的事件，其概率易于计算或是已经有了理论上的假定值，或是根据以往经验已对其值做了充分精确的估计。而我们感兴趣的是一个复杂的事件E，通过种种关系上与简单事件联系起来，以便于用这些简单的事件的概率去算出E的概率。如同对数字做运算，对数字做运算得到新的数字，对事件做运算得出新的事件。
####互斥事件
A/B不能再同一次试验中都发生，则他们是互斥的；对立事件是，非A即B
####事件的和
C=A+B={A发生或B发生}
一定要记住，事件不是已经发生的事情，而是对某种情况的陈述。
####概率加法定理
若干互斥事件之和的概率，等于各事件的概率之和。
$P(A_1+A_2+...)=P(A_1)+P(A_2)+...$  
AB对立事件，则P(A)=1-P(B)     
####事件的积（交）、事件的差
A、B两个事件，则C={A，B都发生}，记做：$A*B，多个的话：\prod_{i=1}^nA_i$   
两个事件差：A-B={A发生，B不发生}
由定义就可以知道，分配、交换、结合都适合
A-B=空，则应该是B事件包含A，而不是B=A，在进行事件计算时需要过一下脑子，按照定义来处理，区别于数字的运算    
####条件概率
条件概率就是附加在一定条件下所计算的概率，一般意义上所有概率都是条件概率，因为即便是基础试验，也会有假定条件。但在概率论中不再加入其他条件或假定，则算出的概率叫做无条件概率。在这个情况下条件概率一般范式就是“已知事件发生情况下。。。”    
条件概率记做P(A|B)，表示B发生了，A发生的概率    
P(A|B) = P(AB)/P(B)，P(B)不为0
####事件的独立性，概率乘法定理
如果P(A|B)=P(A)，则说明B发生对A没有影响，则概率论上称为A和B互相独立。    
这时，P(AB)=P(A)P(B)   
用上面式子做推论，如果上面式子满足，则AB独立。    
$对于N个事件，A_1，A_2，...A_i...，A_N，互相独立，则P(A_1A_2...A_i...A_N)=P(A_1)P(A_2)...P(A_i)...P(A_N)即：P(A_i|P_j...P_n)=P(A_i)$
独立事件任意部分也独立，独立事件组成的事件也独立。    
两两独立的意思是事件任意选两个都是独立的，互相独立的意思是，一群事件都是独立的。    
$$
####全概率公式与贝叶斯公式
__全概率公式__
$$
设事件A1，A2....为无限或有限个事件，他们两两互斥，且每次试验至少有一个事件发生，即\\
A_iA_j=\Phi(不可能事件)，当i\neq j; A1 + A2 + ... = \Omega(必然事件)
$$
有时候称为完备事件群，任意事件和其对立事件组成一个完备事件群。
$$
P(A)=P(A\Omega)=P(AB1+AB2...)=P(AB1) + P(AB2) +...\\

=P(B1)P(A|B1) +P(B2)P(A|B2)+...   --->所谓的全概率公式
$$
在复杂情况下直接计算P(A)可能不方便，但是构造一组事件B，往往可以简化，值得考虑。可以将全概率公式理解为$P(A|B_i)的加权平均和，权值为P(B_i)$    
__贝叶斯公式__
在全概率假设下贝叶斯公式如下：
$P(B_i|A)={P(AB_i)\over P(A)}={P(B_i)P(A|B_i)\over \sum_jP(B_j)P(A|B_j)}$

##随机变量及概率分布
###一维随机变量
随机变量变量按照可能取值性质分为：
1. 离散型随机变量
2. 连续型随机变量
####离散型随机变量的分布
__定义__ 设X为离散型随机变量，其全部值为$\{a_1, a_2...\}$，则：
$$p_i = P(X=a_i), i=1,2,3...\quad\quad\quad 1.1$$ 
称为X的__概率函数__（事件到概率的映射）。
显然有：
$$p_i \ge0,p_1+p_2...=1\quad\quad\quad  1.2$$
1.1公式给出了全部概率1是如何在各个可能值上分配的，也就是概率1在可能值$\{a_1,a_2...\}$上的分布情况，所以经常把1.1称为随机变量x的概率分布。
__定义__ 设X为一随机变量，则函数
$$P(X\le x) = F(x)，-\infty<x<\infty$$
称为X的__分布函数__。
对于离散型随机变量，知道概率函数则：
$$F(x) = P(X\le x)=\sum_{|i:a_i\le x|}p_i$$
这个求和的意思就是只对满足$a_i\le x$的那些i进行。
知道分布函数求概率：$P(x=i)=F(i) - F(i-1)$
分布函数有两个特性：
1. F(X)是单调非降的，$F(x_2) \ge F(x_1), x_2>x_1时$
2. 当$x\to\infty时, F(x)\to 1; 当x\to -\infty时, F(x)\to 0$

<font color="red">__二项分布__</font>
设某事件为A，在某一次试验中发生概率为p，现在独立试验n次，以X表示A发生的次数，考虑事件{X=i}的概率，因为n次试验独立，P(A)=p，则P($\overline A$)=1-p，所以：
$$p_i=C_n^ip^i(1-p)^{n-i}，C_n^i是因为在n次试验中，A事件发生可以占据任何i个位置$$
X所遵循的上述概率分布称为二项分布，一般记作：<font color="red">__B(n,p)__</font>，当X服从某个F分布时，一般表示为__X~F(args)__，所以这个就表示为__X~B(n,p)__
<font color="red">__泊松分布__</font>
若随机变量X的可能取值为0，1,2....，且概率分布为：
$$P(X=i)= {e^{-\lambda}\lambda^i\over i!} $$
也是一种重要的离散分布，常记作：<font color="red">__X~P($\lambda$) __ </font>。它多是出现在当X表示在一定的时间或空间内出现的事件个数这种场合。例如：某段时间发生的交通事故的个数。
一般如果X~B(n,p)，其中n很大，p很小，而<font color="red">__np=$\lambda$__</font>不太大，则X的分布接近于泊松分布P($\lambda$)

####连续随机变量分布
连续随机变量在某一点的概率往往是没意义的，例如射击集中某一点的概率微乎其微可以，因为大部分情况下都会出现或大或小的误差。所以连续随机变量的一个描述方式就是采用概率分布，但更常用__概率密度函数__简称密度函数来刻画。
__定义__ 设连续性随机变量X有概率分布函数F(x)，则F(x)的导数$f(x)=F^\prime(x)$称为X的概率密度函数。
为什么是F(x)的导数？假设取一点x，按照分布函数的定义，事件{x<X<=x+h}的概率为F(x+h)-F(x)，所以[F(x+h)-F(x)]/h的比值在h趋于0时，就是F(x)的导数了。也就是x点处单位长的概率，或者说反映了概率在x点处的密集程度。连续性随机变量X的密度函数f(x)满足：
1. $f(x)\ge 0$
2. $\int_{-\infty}^\infty f(x)dx = 1$
3. 对任何常数a<b，有$P(a\le X\le b)=F(b) - F(a)=\int_a^b f(x)dx$

__正态分布__
如果一个随机变量具有概率密度函数：
$$f(x)= {e^{-(x-\mu)^2\over 2\sigma^2}\over  \sqrt{2\pi}\sigma}，\mu和\sigma取任意常数，0<\sigma^2<\infty$$
则称X为正态随机变量，并记$X$~ $N(\mu，\sigma^2)$
$\mu=0,\sigma^2=1$，则X~N(0, 1)，
设$X_i$服从正态分布，则$X_i^2$构成的随机变量服从卡方分布

__指数分布__
若随机变量X有概率密度函数：
$$
f(x)=\begin{cases}
\lambda e^{-\lambda x},　当x>0；0，当x\le 0
\end{cases}
$$
则x服从指数分部，$\lambda$>0，作为其参数。
指数分布描述了无老化时的寿命分布，但无老化是不可能的，因而只是一种近似，$\lambda$为失效率，失效率越高则平均寿命越小。

__威布尔分布__
上面的指数分布式假设无老化的问题，但实际上这种情况是基本不存在的，实际上失误率应该随使用时间x而上升，不会是常数，假设失效率是x的增函数：$\lambda x^m，\lambda > 0，m >0$，则威布尔分布密度函数和概率分布函数为：
$$
f(x)=  \begin{cases}
\lambda (m+1)x^me^{-\lambda x^{m+1}},　当x>0；0，当x\le 0
\end{cases}\\\
F(x)=1 - e^{-\lambda x^a}，x > 0，F(0) = 0
$$

__均匀分布__
设随机变量X有概率密度函数：
$$
f(x)=\begin{cases}
\lambda {1\over b-a},　当a\le x\le b；0，其他x
\end{cases}
$$
则x服从区间[a,b]上的均匀分布，并常记作：X~R(a, b)

##随机变量的数字特征
随机变量的概率分布是随机变量的概率性质最完整的刻画，而随机变量特征 则是某些由随机变量的分布决定的常数，它刻画了随机变量的某一方面的性质。例如期望、标准差等。
###数学期望（均值）与中位数
####数学期望
$$设随机变量X只取有限个可能值a_1...a_m，期概率分布为P(X=a_i)=p_i，i=1,..., m，则\\\
X的数学期望记为E(X)或EX，定义为E(X)=a_1p_1+...+a_mp_m，随机变量取值的概率平均值$$
如果$\sum_{i=1}^\infty|a_i|p_i\lt \infty$，则$\sum_{i=1}^\infty a_ip_i$为X的数学期望
设X有概率密度函数f(X)，如果：
$$\int_{-\infty}^\infty|x|f(x)dx\lt \infty$$
则称
$$E(X)=\int_{-\infty}^\infty xf(x)dx$$
为X的数学期望。
如果X服从泊松分布，则E(X)=$\lambda$，所以这个参数$\lambda$就是相当于指定时间段中发生的事故的平均次数。
####数学期望的性质
__定理一__
若干随机变量之和的期望等于各个变量期望之和：
$$E(X_1 + X_2 + ... + X_n) = E(X_1) + E(X_2) + ... + E(X_n)$$ 
__定理二__
若干个__独立__随机变量之积的期望，等于各变量的期望之积：
$$E(X_1X_2...X_n)=E(X_1)E(X_2)...E(X_n)$$
__定理三__
设X为随机离散型变量$P(X=a_i)=p_i$ ,i=1,2,...，或者为连续性，有概率密度函数f(x)，则x的函数g(x)的期望为：
$$
离散时：E(g(X))=\sum_i g(a_i)p_i，当\sum_i|g(a_i)|p_i\lt\infty\\

连续时：E(g(X))=\int{-\infty}^\infty g(x)f(x)dx，当\int{-\infty}^\infty|g(x)|f(x)dx<\infty
$$

>三大抽样分布，卡方分布、t分布、F分布
>__卡方分布__是正态分布随机事件的平方所构成的事件，期望为n（n>1)
>__t分布式__，设$X_1$服从标准正态分布N(0,1)，$X_2$服从自由度为n的卡方分布，且$X_1,X_2$互相独立，则称变量$t={X_1\over {X_2\over n}^{1\over2}}$服从的分布为自由度为n的t分布。
>__F分布__，设$X_1$服从自由度为m的正态分布，$X_2$服从自由度为n的正态分布，且$X_1,X_2$互相独立，则称变量F=$X_1/m\over X_2/n$所服从分布为F分布。

####条件数学期望（条件均值）
随机变量Y的条件数学期望就是它在给定的某种附加条件下的数学期望。这个的含义就是随着x的变化，y的平均值的变化，这个还是蛮有用的，因为研究的时候经常使用，随着受教育年数x的变化，其平均收入如何等。
类似全概率公式：
$$P(A)=\sum_iP(B_i)P(A|B_i)，P(B_i)可以理解为一种加权，而这个和也可以理解为一种加权平均$$
也可以利用条件数学期望求某个随机变量的期望：
$$E(Y)=\int_{-\infty}^\infty E(Y|x)f_1(x)dx，f_1(x)是X在x处的概率密度\\\
如果X、Y都是离散的话：E(Y)=\sum_{i=1}^\infty p_iE(Y|a_i)$$
####中位数
假设连续随机变量X的分布函数是F(X)，则满足条件：
$$P(X\le m)=F(m)=1/2$$
的数m称为x或分布F的中位数。就是在概率上来讲，将数据一分为二。
由于中位数没有期望上面那么多的数学特性（例如两个随机变量之和的中位数没有简单的数学关系），而且中位数可能不唯一、不存在，所以中位数没有期望那么应用广泛。
###方差与矩
####方差和标准差
__方差、标准差定义__
$$Var(X)=E[(X-E(X))^2]=E(X^2-2E(X)X+E^2(X))=E(X^2)-E^2(X)，\\\ X为随机变量，\sqrt{Var(X)}称为标准差$$

__定理一__：
1. 常数的的方差为0
2. 如果C为常数，则$Var(X+C) = Var(X)$
3. 如果C为常数，则$Var(CX) = C^2Var(X)$

__定理二__:
$$X_1,X_2...X_n为独立随机变量，Var(X_1+V_2+...+V_n)=Var(X_1) + ... + Var(X_n)$$

####矩
__定义__
设X为随机变量，c为常数，k为正整数，则量$E[(X-c)^k]$称为X关于c点的k阶矩。
有两个情况：
1. c=0，这时a_k=E(X^k)称为X的k阶原点矩
2. c=E(X)，这时，$u_k=E[(X-E(X))^k]$称为X的k阶中心矩。

一阶原点矩就是期望，一阶中心矩$u_1=0$，二阶中心矩$u_2$是X的方差Var(X)。$u_3$经常用来衡量分布是否有偏，如果$u_3$>0，则分布为正偏或右偏，如果$u_3$<0，则分布为负偏或左偏。$u_3$=0，则就是对称的，例如正态分布。
__偏度系数__：$\beta_1=u_3/u_2^{3/2}$
$u_4$用来衡量密度分布在均值附近的陡峭程度，$u_4$小的时候则说明x取值在概率上集中在E(X)附近，也就是不那么陡峭。
__峰度系数__：$\beta_2=u_4/u_2^2$ 对于正态分布有常数的峰度系数，解释见P133
###协方差与相关系数
协方差和相关系数是多维随机向量中，反应分量之间的关系。
__定义__： $E[(X-E(X))(Y-E(Y))]$为X,Y的协方差，并记作$Cov(X,Y)$
几个简单的性质：
1. $Cov(c_1X+c_2, c_3Y+c_4)=c_1c_2Cov(X,Y)$
2. $Cov(X,Y)=E(XY)-E(X)E(Y)$

__定理__：
1. $如果X,Y独立，则Cov(X,Y)=0$
2. $[Cov(X,Y)]^2\le Var(X)Var(Y)，当且仅当X和Y有严格的线性关系时等号才成立（例如Y=aX+b)$

__定义__：$Cov(X,Y)/(\sigma_1\sigma_2)为X,Y的相关系数，记作Corr(X,Y)，\sigma_1为X的标准差，\sigma_2为Y的标准差$

__定理__：
1. $如果X,Y独立，则Corr(X,Y)=0$
2. $-1\le Corr(X,Y)\le 1，或|Corr(X,Y)|\le 1，等号当且仅当X,Y严格线性相关时达到。$

几个重要的结论：
1. 当Corr(X,Y)=0不一定X,Y就是独立的，定理1中的结论不可逆
2. 相关系数也常称为__线性相关系数__，因为只有X,Y有某种严格的线性相关时，这个值才能达到最大值1，X和Y即便是具有某种函数关系但非线性关系，则其值不进不必为1还可以为0。
3. 如果0<|Corr(X,Y)|<1，则解释为X,Y之间有__一定程度的线性关系__而非严格的线性关系。一定程度的意思是，这种线性关系可能反应在区域内，X的值虽然不能确定Y的值，但是能够看出X确定，Y肯定是在某个窄小的范围内，这个范围越小，说明'一定'的程度越深。 
4. 线性相关性也可以用最小二乘法解释，设两个随机变量X线性函数aX+b逼近Y，如何选取a,b才能更好的逼近Y呢？使用最小二乘的方法：求使$E[(Y-a-bX)^2]$达到最小a,b值。如果记$L(X)=aX+b$的话，那可以得到$$L(X)=E(Y)-Var(X)^{-1}Var(Y)Corr(X,Y)E(X)+Var(X)^{-1}Var(Y)X$$$所以E(Y-L(X))^2=0$则Y=L(X)说明Y和X就是有线性关系，这段推论很重要，详见P138
  那以后可以首先计算变量之间的相关系数判断其相关性，然后再使用算法来求得线性关系的参数？

由于相关系数只能刻画线性关系，所以概率论中引入了另一些相关性指标，但是这些指标在实际应用中比较少见。因为其一，这些指标太过复杂；其二，在统计学应用上，最重要的二维分布是二维正态分布，而二维正态分布而言，相关系数就是X,Y的相关性的一个完美刻画。有一点，如果X,Y为二维正态分布，则即允许使用任何M(X)去逼近（还是最小二乘方法$E[(Y-M(X))^2]$)，所以得到的还是L(X)。
###大数定理和中心极限定理
在概率论中，习惯于把和的分布收敛于正态分布的那一类定理都叫做中心极限定理。而且在很一般的情况下，和的极限分布就是正态分布，也是将问题极限化。
####大数定理
频率收敛于概率
在不断的试验中，我们能够取得的实际的次数的平均值概率收敛于可能存在的真实值
__定理__：
设$X_1,X_2...Xn...$是独立同分布的随机变量，记他们的公共均值为a，又设他们的方差存在并记为$\sigma^2$，则对任意$\epsilon\gt0$有：
$$lim_{n\to\infty}P(|\overline X - a|\ge \epsilon) = 0$$
这句话的意思是，当n取极限时，$\overline X$与a的偏差会很小，基本就是不可能事件（所以概率为0）$\overline X$是X事件的平均值。又称为伯努利大数定理。
__马尔科夫不等式__  若Y为只取非负值的随机变量，则对任给常数$\epsilon>0$有：
$$P(Y\ge \epsilon) \le E(Y)/\epsilon$$
设Y为连续型变量，密度函数为f(Y)，因为Y只取非负数，有f(y)=0，当y=0，故：
$$E(Y)=\int_0^\infty yf(y)dy\ge\int_\epsilon^\infty yf(y)dy$$
因为在$[\epsilon,\infty)$内总有$y\ge\epsilon$，且$\int_\epsilon^\infty f(y)dy就是P(Y\ge\epsilon)$，故：$$E(Y)\ge\int_\epsilon^\infty yf(y)dy\ge\epsilon\int_\epsilon^\infty f(y)dy=\epsilon P(Y\ge\epsilon)$$

__契比雪夫不等式__ 若Var(Y)存在，则$$P(|Y-EY|\ge\epsilon)\le\epsilon^2$$

####中心极限定理
__定理__ 设$X_1,X_2...X_n...$为独立同分布的随机变量，$E(X_i)=a，Var(X_i)=\sigma^2，0<\sigma<\infty$，则对任何实数x，有：
$$\lim_{n\to\infty}P({1\over\sqrt{n}\sigma}(X_1+X_2+...+X_n-na)\le x)=\Phi(x)$$
这里$\Phi(x)$是标准正态分布__N(0,1)__的分布函数，即：
$$\Phi(x)={1\over\sqrt{2\pi}}\int_{-\infty}^\infty e^{-t^2/2}dt$$
上上面的式子$(X_1+X_2+...+X_n-na)/(\sqrt n\sigma)$可以把$X_1+X_2+...+X_n$看做一个随机变量，其方差为$n\sigma^2$，期望为$na$，所以这个式子相当于对$X_1+X_2+...+X_n$的标准化（减去均值除以标准差）  
上面这个定理通称为林德伯格定理或林德伯格-莱维定理。 
##参数估计
###数理统计学
概率论是数理统计的基础，数理统计是概率论的应用。
####数理统计的意义
随机误差：由于数据产生的随机性导致实验结果产生偏差的误差
__数理统计__就是使用概率论和数学的方法，研究怎样收集（通过实验或观察） 带有随机误差的数据，并在设定模型（称为统计模型）之下，对这种数据进行分析（称为统计分析），以对所研究问题作出推断（称为统计推断）
例如：工厂生产的元件服从指数分布，其平均寿命就是$1/\lambda$，一般$\lambda$都是未知，所以可以通过抽取一部分元件测其平均值来估计$\lambda$，这个就算是参数估计。而根据这个估计值来做决定，叫做假设检验问题。例如根据这个估计值来判断某一批元件平均寿命为l是否达到标准。
####总体
总体是指与所研究的问题有关的对象(个体)的全体所构成的集合。例如：研究某校大学生成绩，整体就是全校学生，个体就是某个学生。
赋予一定概率分布的总体就称为__统计总体__。同样一堆数据，A和B的不同就是通过其概率分布来体现，不同的概率分布可能采取不同的分析方式。如果概率分布不知道，一堆数据要如何处理？
所以在数理统计学中总体这个概念的要旨——__总体就是一个概率分布。__ 但是概率分布会有一些未知参数，所以总体分布就是一个概率分布族的一员。 这种知道总体分布，而参数未知的叫做参数分布、参数总体。而对总体分布都不知道的，就不能通过若干个位置参数表达出来，这种情况称为非参数总体。将总体无线化可以将离散问题转化成连续问题，这样就可以更方便的使用概率论来理解、分析总体分布。而且随着n越大，两者之间的误差会越来越小。
####样本
样本是按照一定规定（每个个体等可能被抽）在总体中抽出的一部分个体。
根据总体的数量需要在意有放回抽样和不放回抽样。
####统计量
完全由样本决定的量，叫做统计量。它不能依赖于总体分布中所包含的位置参数。
统计量可以看做是对样本的一种加工，例如对抽取的样本计算平均值。
__样本方差__：$S^2={1\over n-1}\sum_{i=1}^n(X_i-\overline X)^2$
__样本矩__：$a_k=(X_1^k+X_2^k+...X_n^k)/n为k阶样本中心矩；m_k=\sum_{i=1}^n(X_i-\overline X)^k/n 为k阶样本中心矩$
###矩估计、极大似然估计和贝叶斯估计
####参数的点估计问题
设有一个统计总体，以$f(x, \theta_1,...,\theta_k)$记其概率密度函数（若总体分布为连续型）或其概率函数（总体分布为离散型）。这分布包含k个位置参数$\theta_1,...,\theta_k$，例如正态分布：
$$f(x, \theta_1, \theta_2)=(\sqrt{2\pi\theta_2})^{-1}e^{-{1\over 2\theta_2}(x-\theta_1)^2}$$
参数估计问题，设从总体里面抽出样本$X_1,...,X_n$，要依据这些样本去对参数$\theta_1,...,\theta_k$的未知值做出估计。当然我们也可以只要求估计$\theta_1,...,\theta_k$中的一部分，或估计它们的某个函数$g(\theta_1,...,\theta_k)$，为要估计$\theta_1$，我们需要构造出适当的统计量$\hat \theta_1=\hat \theta_1(X_1,...,X_n)$，用作$\theta_1$的估计值。为着这样特定目的构造的统计量$\theta_1$，叫做（$\theta_1$的）估计量。由于未知参数$\theta_1$是数轴上的一个点，用$\hat\theta_1$估计$\theta_1$等于用一个点去估计另一个点，所以这样的估计叫做点估计，区别于下面的区间估计。
由于估计方法有多种，所以需要有必要的方法来评判估计的好坏。
####矩估计法
设总体分布为$f(x, \theta_1,...,\theta_k)$，则它的矩（原点矩和中心矩都可以，此处以原点矩为例）
$$a_m=\int_{-\infty}^\infty x^mf(x,\theta_1,...,\theta_k)dx，（或\sum_i x_i^mf(x_i,\theta_1,...,\theta_k)$$
依赖于$\theta_1,...,\theta_k$。另一方面，至少在样本大小n较大时，$a_m$又应接近于样本原点矩$a_m$:
$$a_m=a_m(\theta_1,...,\theta_k)\approx a_m=\sum_{i=1}^n X_i^m/n$$
这样就得到方程组（$X_i,...,X_n$就是样本），得到$\hat\theta_i=\hat\theta_i(X_1,...,X_n)$。
__其实就是通过矩这个统计量，以及样本的值来估计整体值。__
####极大似然估计
设总体有分布$f(X;\theta_1,...,\theta_n),X_1,...,X_n$为自总体中抽出的样本，则样本$(X_1,...,X_n)$的分布为（概率密度函数），
$$f(X_1;\theta_1,...,\theta_k)f(X_2;\theta_1,...,\theta_k)...f(X_n;\theta_1,...,\theta_k)$$
记之为$L(X_1,...,X_n;\theta_1,...,\theta_n)$。
应该用似然程度最大的那个点$(\theta_1, \theta_2, ..., \theta_k)$，即满足条件：
$$L(X_1,...,X_n;\theta_1^*,...,\theta_k^*)=\max_{\theta_1,...,\theta_k}L(X_1,...,X_n;\theta_1,...,\theta_k)$$
的$(\theta_1^*,...,\theta_k^*)$作为$(\theta_1,...,\theta_k)$的估计值。
因为$$\log L=\sum_{i=1}^n log f(X_i;\theta_1,...,\theta_k)，等式成立参见上面的密度函数等式$$
且为使L达到最大，只须使log L达到最大，所以在f对$\theta_1,...,\theta_k$存在__连续偏导数__时，可建立方程组(似然方程组)：$${d\log L\over d\theta_i}=0，i=1,...,k$$
如果这组方程有唯一解，又能验证它是一个极大值点。
如果极大似然估计和矩估计得到结果相同，则这些估计就是良好的。
样本中位数：如果样本数量是奇数，则为中间的样本；偶数则为中间两个样本的平均数。
####贝叶斯法
上面两种方式都是假设抽样之前对参数没有任何了解，而贝叶斯法则认为抽样之前我们已经对参数有了一定的知识——先验知识。而且这种先验知识必须用这些参数的某种概率分布表达数来（参数的概率密度函数），这些概率分布就叫做__参数的先验分布__或__先验密度__。这些分布总结了我们在试验之前对参数的知识。
如果这个分布你不知道，贝叶斯的观点就是你可以主观假设（这个有点。。。）
已定下了先验密度之后，计算参数$\theta$的估计：


